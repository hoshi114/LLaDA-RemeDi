# LLaDA-RemeDi: Re-masking Mechanism for Diffusion LLMs

> **Unofficial Implementation of [RemeDi](https://arxiv.org/abs/2509.23653)**
> Based on the [LLaDA](https://github.com/ML-GSAI/LLaDA) architecture.

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/mit)
[![Status](https://img.shields.io/badge/Status-In%20Progress-yellow)](https://github.com/)
[![Framework](https://img.shields.io/badge/Framework-PyTorch%20|%20HuggingFace-red)](https://pytorch.org/)

## üìñ Introduction

This repository implements a **Re-masking Mechanism (RemeDi)** for Mask-based Diffusion Language Models. The core idea is to introduce an **Unmasking Policy Stream (UPS)** alongside the standard Token Prediction Stream (TPS), enabling the model to dynamically re-mask and correct erroneous tokens during the generation process.

This project serves as a technical assignment to explore:
- Dual-stream architecture implementation (TPS + UPS).
- Supervised Fine-Tuning (SFT) for re-masking policies.
- Data construction for diffusion-based correction tasks.

---

## üöÄ Methodology

### 1. Architecture Design
The model extends the standard `LLaDA` backbone by adding a lightweight **UPS Head**.
- **TPS (Token Prediction Stream):** Predicts the original tokens from masked inputs (Standard Diffusion).
- **UPS (Unmasking Policy Stream):** A binary classification head that predicts the `re-masking probability` for each token.
    - Input: Hidden states from the transformer backbone.
    - Output: A score indicating whether a token should be kept or re-masked.

![architecture](/imgs/architecture.png)

### 2. Training Pipeline
- **Dataset:** Synthetic data generated by corrupting standard text datasets (e.g., S1K / MATH-500).
- **Noise Injection:** Simulating "intermediate errors" during diffusion to train the UPS head.
- **Loss Function:** Joint optimization of `Loss_TPS` and `Loss_UPS`.

---

## üõ†Ô∏è Quick Start (Planned)

This repo integrates a RemeDi UPS head on top of the LLaDA backbone and reuses the SMDM masked‚Äëdiffusion training shell for Remask SFT. The following reflects the near‚Äëterm plan and interfaces.

### Requirements
```bash
git clone https://github.com/your-username/LLaDA-RemeDi.git
cd LLaDA-RemeDi
pip install -r requirements.txt
```

### Data & Datasets
- Primary (bring‚Äëup): `simplescaling/s1K-1.1`
- Math (reasoning/visualization): `HuggingFaceH4/MATH-500`
- Code/Instruction: `OpenCoder-LLM/opc-sft-stage2` with subsets `educational_instruct`, `evol_instruct`, `mceval_instruct`
- Optional (eval‚Äëonly): GSM8K, HumanEval/MBPP

Remask SFT performs in‚Äëcode noise injection per RemeDi: mask noise œÅ_mask(t)=t and incorrect‚Äëtoken noise œÅ_incorrect(t)=4 r t (1‚àít) on the answer span; the prompt remains clean.

### Training (Remask SFT)
We will provide a minimal script that reuses the SMDM trainer:
```bash
# Upcoming minimal trainer (freeze backbone; train UPS head)
python train_remask_sft.py \
  --dataset simplescaling/s1K-1.1 \
  --model GSAI-ML/LLaDA-8B-Base \
  --mask_id 126336 \
  --r_incorrect 0.1 \
  --seq_len 1024
```
Notes:
- Loss = diffusion CE on masked positions + UPS BCE on all positions (masked labels use stop‚Äëgrad pŒ∏(x0|xt)).
- Initial phase trains only the UPS head; LoRA on the backbone is optional later.

### Inference
Dynamic remasking is enabled in the sampler with a selectable confidence source:
- `ups`: sigmoid(h) from the UPS head (recommended after SFT)
- `tps_prob`: pŒ∏(xÃÇ | x_t) from TPS (current baseline)
- `random`: uniform baseline

OpenCompass wrapper threads this flag into `generate()`; EOS/EoT gating and CFG remain available.

---

## üìù Progress & Analysis

### Current Progress
- [x] Project initialization and plan (`RemeDi_Plan.md`)
- [ ] Phase 1: UPS head wrapper + sampler dynamic remask (confidence_source)
- [ ] Phase 2: Remask SFT (SMDM shell; freeze backbone; train UPS head)
- [ ] Phase 3: Targeted eval (MATH‚Äë500/OPC; optional GSM8K/HumanEval subsets)
- [ ] Phase 4: Visualizations and ablations (mask‚Äëcount monotonicity, r/steps/block_length)

### Notes
- Maintain monotonic mask schedule by aligning per‚Äëstep top‚ÄëK with block sampling and answer span; non‚Äëselected answer tokens are re‚Äëmasked each step.
- Preserve fallbacks: if UPS is unavailable, use `tps_prob` or `random` to keep demos and evaluation scripts functioning.

---

## üìö References
- **RemeDi Paper:** [arXiv:2509.23653](https://arxiv.org/abs/2509.23653)
- **LLaDA Paper:** [arXiv:2502.09992](https://arxiv.org/abs/2502.09992)
- **Base Code:** [ML-GSAI/LLaDA](https://github.com/ML-GSAI/LLaDA)

---

*Author: Hoshi*
*Last Updated: Nov 2025*
