# LLaDA-RemeDi: Re-masking Mechanism for Diffusion LLMs

> **Unofficial Implementation of [RemeDi](https://arxiv.org/abs/2509.23653)**
> Based on the [LLaDA](https://github.com/ML-GSAI/LLaDA) architecture.

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/mit)
[![Status](https://img.shields.io/badge/Status-In%20Progress-yellow)](https://github.com/)
[![Framework](https://img.shields.io/badge/Framework-PyTorch%20|%20HuggingFace-red)](https://pytorch.org/)

## üìñ Introduction

This repository implements a **Re-masking Mechanism (RemeDi)** for Mask-based Diffusion Language Models. The core idea is to introduce an **Unmasking Policy Stream (UPS)** alongside the standard Token Prediction Stream (TPS), enabling the model to dynamically re-mask and correct erroneous tokens during the generation process.

This project serves as a technical assignment to explore:
- Dual-stream architecture implementation (TPS + UPS).
- Supervised Fine-Tuning (SFT) for re-masking policies.
- Data construction for diffusion-based correction tasks.

---

## üöÄ Methodology

### 1. Architecture Design
The model extends the standard `LLaDA` backbone by adding a lightweight **UPS Head**.
- **TPS (Token Prediction Stream):** Predicts the original tokens from masked inputs (Standard Diffusion).
- **UPS (Unmasking Policy Stream):** A binary classification head that predicts the `re-masking probability` for each token.
    - Input: Hidden states from the transformer backbone.
    - Output: A score indicating whether a token should be kept or re-masked.

![architecture](/imgs/architecture.png)

### 2. Training Pipeline
- **Dataset:** Synthetic data generated by corrupting standard text datasets (e.g., S1K / MATH-500).
- **Noise Injection:** Simulating "intermediate errors" during diffusion to train the UPS head.
- **Loss Function:** Joint optimization of `Loss_TPS` and `Loss_UPS`.

---

## üõ†Ô∏è Quick Start

This repo integrates a RemeDi UPS head on top of the LLaDA backbone and reuses a minimal masked‚Äëdiffusion trainer for Remask SFT.

### Requirements
```bash
git clone https://github.com/your-username/LLaDA-RemeDi.git
cd LLaDA-RemeDi
# minimal training deps
pip install -r requirements-train.txt
```

### Data & Datasets
- Primary (bring‚Äëup): `simplescaling/s1K-1.1`
- Math (reasoning/visualization): `HuggingFaceH4/MATH-500`
- Code/Instruction: `OpenCoder-LLM/opc-sft-stage2` with subsets `educational_instruct`, `evol_instruct`, `mceval_instruct`
- Optional (eval‚Äëonly): GSM8K, HumanEval/MBPP

Remask SFT performs in‚Äëcode noise injection per RemeDi: mask noise rho_mask(t)=t and incorrect‚Äëtoken noise rho_incorrect(t)=4 r t (1‚àít) on the answer span; the prompt remains clean.

### Training (Remask SFT)
Freeze backbone; train the UPS head only.
```bash
# s1K bring-up (1 epoch)
python remedi/train_remask_sft.py \
  --model_name GSAI-ML/LLaDA-8B-Base \
  --dataset simplescaling/s1K-1.1 \
  --seq_len 1024 --batch_size 1 --epochs 1 \
  --lr 1e-4 --lambda_ups 1.0 --r_incorrect 0.1 \
  --mask_id 126336 \
  --save_path checkpoints/ups_head_s1k_len1024_b1_r010_l1p0_linear.pt

# continue training on MATH-500 (adapt policy)
python remedi/train_remask_sft.py \
  --model_name GSAI-ML/LLaDA-8B-Base \
  --dataset HuggingFaceH4/MATH-500 --split test \
  --seq_len 1024 --batch_size 1 --epochs 1 \
  --lr 3e-5 --lambda_ups 1.0 --r_incorrect 0.1 \
  --mask_id 126336 \
  --load_ups_head checkpoints/ups_head_s1k_len1024_b1_r010_l1p0_linear.pt \
  --save_path checkpoints/ups_head_s1k_math_len1024_b1_r010_l1p0_linear.pt
```
Notes:
- Loss = diffusion CE on masked positions + UPS BCE on all positions (masked labels use stop‚Äëgrad p(x0|xt)).
- Initial phase trains only the UPS head; LoRA on the backbone is optional later.

### Training (LoRA TPS + UPS)
Enable TPS+UPSËÅîÂêàËÆ≠ÁªÉÔºö‰ΩøÁî® LoRA Ë∞É‰ºòÈ™®Êû∂ÔºàTPSÔºâÔºåÂêåÊó∂ÁªßÁª≠ËÆ≠ÁªÉ UPS Â§¥ÔºàBCE ÁöÑÊ¢ØÂ∫¶‰∏çÂõû‰º†Âà∞È™®Êû∂Ôºâ„ÄÇ
```bash
# joint Ê®°ÂºèÔºàÊé®ËçêÔºâ
python remedi/train_remask_lora.py \
  --model_name GSAI-ML/LLaDA-8B-Base \
  --dataset simplescaling/s1K-1.1 \
  --seq_len 1024 --batch_size 1 --epochs 1 --grad_accum 2 \
  --lora_r 16 --lora_alpha 32 --lora_dropout 0.05 --lora_target auto \
  --lr_lora 5e-5 --lr_ups 1e-4 \
  --lambda_ups 1.0 --r_incorrect 0.1 --mask_id 126336 \
  --save_lora_dir checkpoints/lora_s1k \
  --save_ups_head checkpoints/ups_head_s1k_lora.pt

# alternating Ê®°ÂºèÔºàTPS:UPS=4:1Ôºâ
python remedi/train_remask_lora.py \
  --model_name GSAI-ML/LLaDA-8B-Base \
  --dataset HuggingFaceH4/MATH-500 --split test \
  --seq_len 1024 --batch_size 1 --epochs 1 \
  --train_mode alternating --alt_ratio 4 \
  --load_lora_dir checkpoints/lora_s1k \
  --load_ups_head checkpoints/ups_head_s1k_lora.pt \
  --save_lora_dir checkpoints/lora_math \
  --save_ups_head checkpoints/ups_head_math_lora.pt
```

### Inference (Compare)
Dynamic remasking is enabled in the sampler with a selectable confidence source:
- `ups`: sigmoid(h) from the UPS head (recommended after SFT)
- `tps_prob`: pŒ∏(xÃÇ | x_t) from TPS (current baseline)
- `random`: uniform baseline
```bash
python remedi/infer_compare.py \
  --model_name GSAI-ML/LLaDA-8B-Base \
  --ups_head checkpoints/ups_head_s1k_math_len1024_b1_r010_l1p0_linear.pt \
  --lora_path checkpoints/lora_math \
  --prompt "Give a brief plan for learning calculus." \
  --prompt "Write a Python function to reverse a string." \
  --steps 128 --gen_length 128 --block_length 16 \
  --temperature 0.0 --cfg_scale 0.0 --mask_id 126336 \
  --logits_eos_inf --confidence_eos_eot_inf
```
See also `Local_RUN.md` and `Remask_SFT_RUN.md`.

### Evaluation (GSM8K/MATH)
```bash
# GSM8K 50-sample eval (numeric accuracy)
python remedi/eval_gsm_math.py \
  --model_name GSAI-ML/LLaDA-8B-Base \
  --ups_head checkpoints/ups_head_s1k_math_gsm8k_len1024_b1_r010_l1p0_linear.pt \
  --lora_path checkpoints/lora_math \
  --dataset openai/gsm8k --subset main --split test --format gsm8k \
  --max_samples 50 --infer_bs 1 \
  --steps 128 --gen_length 128 --block_length 16 \
  --temperature 0.0 --cfg_scale 0.0 --mask_id 126336 --eos_gating
```

---

## üìä Results (Preliminary)

- GSM8K 50-sample (Base, steps=128, gen_length=128, block_length=16, eos/eot gating):
  - ups acc=0.360, tps_prob acc=0.260, random acc=0.120
  - All methods extracted answers (coverage=1.0).

These results indicate the trained UPS head improves unmasking decisions over the TPS-probability heuristic on math word problems. Full-scale evaluation is left to future work.

## ‚úÖ Status

- [x] Phase A: Sampler dynamic remask + UPS head scaffold
- [x] Phase B: Remask SFT (s1K ‚Üí MATH-500/GSM8K continued training)
- [x] Phase C: Small-sample evaluation (GSM8K) and inference comparison
- [ ] Phase D: Scaling & ablations (r_incorrect, lambda_ups, ups_width, seq_len)

---

## üìö References
- **RemeDi Paper:** [arXiv:2509.23653](https://arxiv.org/abs/2509.23653)
- **LLaDA Paper:** [arXiv:2502.09992](https://arxiv.org/abs/2502.09992)
- **Base Code:** [ML-GSAI/LLaDA](https://github.com/ML-GSAI/LLaDA)

---

*Author: Hoshi*
*Last Updated: Nov 2025*
