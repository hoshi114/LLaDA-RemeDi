# LLaDA-RemeDi: Re-masking Mechanism for Diffusion LLMs

> **Unofficial Implementation of [RemeDi](https://arxiv.org/abs/2509.23653)**
> Based on the [LLaDA](https://github.com/ML-GSAI/LLaDA) architecture.

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/mit)
[![Status](https://img.shields.io/badge/Status-In%20Progress-yellow)](https://github.com/)
[![Framework](https://img.shields.io/badge/Framework-PyTorch%20|%20HuggingFace-red)](https://pytorch.org/)

## üìñ Introduction

This repository implements a **Re-masking Mechanism (RemeDi)** for Mask-based Diffusion Language Models. The core idea is to introduce an **Unmasking Policy Stream (UPS)** alongside the standard Token Prediction Stream (TPS), enabling the model to dynamically re-mask and correct erroneous tokens during the generation process.

This project serves as a technical assignment to explore:
- Dual-stream architecture implementation (TPS + UPS).
- Supervised Fine-Tuning (SFT) for re-masking policies.
- Data construction for diffusion-based correction tasks.

---

## üöÄ Methodology

### 1. Architecture Design
The model extends the standard `LLaDA` backbone by adding a lightweight **UPS Head**.
- **TPS (Token Prediction Stream):** Predicts the original tokens from masked inputs (Standard Diffusion).
- **UPS (Unmasking Policy Stream):** A binary classification head that predicts the `re-masking probability` for each token.
    - Input: Hidden states from the transformer backbone.
    - Output: A score indicating whether a token should be kept or re-masked.

![architecture](/imgs/architecture.png)

### 2. Training Pipeline
- **Dataset:** Synthetic data generated by corrupting standard text datasets (e.g., S1K / MATH-500).
- **Noise Injection:** Simulating "intermediate errors" during diffusion to train the UPS head.
- **Loss Function:** Joint optimization of `Loss_TPS` and `Loss_UPS`.

---

## üõ†Ô∏è Quick Start (Planned)

> *Note: The following commands represent the planned usage and directory structure.*

### Requirements
```bash
git clone https://github.com/your-username/LLaDA-RemeDi.git
cd LLaDA-RemeDi
pip install -r requirements.txt
```

### Data Preparation
Generate synthetic training data with UPS labels:
```bash
# Planned script for data generation
python scripts/prepare_data.py --config configs/remedi_data.yaml
```

### Training (SFT)
Fine-tune the LLaDA model with the RemeDi head:
```bash
# Planned script for SFT training
python train.py --config configs/sft_remedi.yaml
```

---

## üìù Progress & Analysis

### Current Progress
- [x] **Project Initialization**: Setup repository and environment.
- [ ] **Phase 1: Architecture** - Integrate UPS Head into LLaDA backbone.
- [ ] **Phase 2: Data Pipeline** - Implemente noise injection strategy for UPS labels.
- [ ] **Phase 3: SFT** - Training loop setup on AutoDL (A100/4090).
- [ ] **Phase 4: Analysis** - Verify re-masking behavior on test cases.

### Challenges & Solutions (Thinking Process)

*(This section records the engineering journey and problem-solving process)*

*(To be updated during development)*

---

## üìö References
- **RemeDi Paper:** [arXiv:2509.23653](https://arxiv.org/abs/2509.23653)
- **LLaDA Paper:** [arXiv:2502.09992](https://arxiv.org/abs/2502.09992)
- **Base Code:** [ML-GSAI/LLaDA](https://github.com/ML-GSAI/LLaDA)

---

*Author: Hoshi*
*Last Updated: Nov 2025*